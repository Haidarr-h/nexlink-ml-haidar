{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59e8266-8a69-45b8-a954-c9db4426cb66",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "Project Deadline: 12/12/24\n",
      "Project Duration: 35 hours\n",
      "Project Start Date: 10/06/24\n",
      "Project End Date: 13/06/24\n",
      "It is feasible and achievable\n",
      "\n",
      "Worker Schedules:\n",
      "Schedule for UserID1:\n",
      "  10/06/24: Implementasi Fitur_0 (8 hours)\n",
      "  10/06/24: Pengujian dan Perbaikan_2 (8 hours)\n",
      "  11/06/24: Implementasi Fitur_0 (8 hours)\n",
      "  11/06/24: Pengujian dan Perbaikan_2 (8 hours)\n",
      "  12/06/24: Implementasi Fitur_0 (8 hours)\n",
      "  12/06/24: Pengujian dan Perbaikan_2 (8 hours)\n",
      "  13/06/24: Implementasi Fitur_0 (1 hours)\n",
      "  13/06/24: Pengujian dan Perbaikan_2 (8 hours)\n",
      "  14/06/24: Pengujian dan Perbaikan_2 (3 hours)\n",
      "Schedule for UserID2:\n",
      "  10/06/24: Implementasi Fitur_1 (8 hours)\n",
      "  11/06/24: Implementasi Fitur_1 (8 hours)\n",
      "  12/06/24: Implementasi Fitur_1 (8 hours)\n",
      "  13/06/24: Implementasi Fitur_1 (1 hours)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, deque\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print('gpu ', gpu)\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model(\"text_classify.h5\")\n",
    "\n",
    "# Load tokenizer and label encoder\n",
    "with open('tokenizer.pkl', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('label_encoder.pkl', 'rb') as handle:\n",
    "    label_encoder = pickle.load(handle)\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "# Define your max_length\n",
    "max_length = 100  # Adjust this based on your preprocessing\n",
    "\n",
    "def predict_task_labels(model, tokenizer, label_encoder, tasks):\n",
    "    sequences = tokenizer.texts_to_sequences(tasks)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "    predictions = model.predict(padded_sequences)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    predicted_labels = label_encoder.inverse_transform(predicted_classes)\n",
    "    return predicted_labels\n",
    "\n",
    "def get_task_durations(task):\n",
    "    data = pd.read_csv('dataset5.csv')\n",
    "    data_choice = data[data['Label_Task'] == task]\n",
    "    if data_choice.empty:\n",
    "        raise ValueError(f\"Task '{task}' not found in the dataset.\")\n",
    "    duration = data_choice['Estimated_hours'].iloc[0]\n",
    "    return duration\n",
    "\n",
    "def assign_workers_to_tasks(task_labels, task_workers):\n",
    "    worker_assignments = {}\n",
    "    for idx, (task, worker) in enumerate(zip(task_labels, task_workers)):\n",
    "        unique_task = f\"{task}_{idx}\"  # Ensure each task is unique\n",
    "        duration = get_task_durations(task)\n",
    "        worker_assignments[unique_task] = (worker, duration)\n",
    "    return worker_assignments\n",
    "\n",
    "def calculate_earliest_times(tasks, dependencies):\n",
    "    earliest_start = {task: 0 for task in tasks}\n",
    "    earliest_finish = {task: duration for task, (worker, duration) in tasks.items()}\n",
    "    \n",
    "    adj_list = defaultdict(list)\n",
    "    in_degree = {task: 0 for task in tasks}\n",
    "    \n",
    "    for task, deps in dependencies.items():\n",
    "        for dep in deps:\n",
    "            adj_list[dep].append(task)\n",
    "            in_degree[task] += 1\n",
    "    \n",
    "    topo_order = []\n",
    "    zero_in_degree_queue = deque([task for task in tasks if in_degree[task] == 0])\n",
    "    \n",
    "    while zero_in_degree_queue:\n",
    "        task = zero_in_degree_queue.popleft()\n",
    "        topo_order.append(task)\n",
    "        \n",
    "        for neighbor in adj_list[task]:\n",
    "            in_degree[neighbor] -= 1\n",
    "            if in_degree[neighbor] == 0:\n",
    "                zero_in_degree_queue.append(neighbor)\n",
    "    \n",
    "    for task in topo_order:\n",
    "        for neighbor in adj_list[task]:\n",
    "            earliest_start[neighbor] = max(earliest_start[neighbor], earliest_finish[task])\n",
    "            earliest_finish[neighbor] = earliest_start[neighbor] + tasks[neighbor][1]\n",
    "    \n",
    "    return earliest_start, earliest_finish\n",
    "\n",
    "def calculate_latest_times(tasks, dependencies, project_duration):\n",
    "    latest_finish = {task: project_duration for task in tasks}\n",
    "    latest_start = {task: project_duration - duration for task, (worker, duration) in tasks.items()}\n",
    "    \n",
    "    adj_list = defaultdict(list)\n",
    "    for task, deps in dependencies.items():\n",
    "        for dep in deps:\n",
    "            adj_list[task].append(dep)\n",
    "    \n",
    "    for task in reversed(list(tasks.keys())):\n",
    "        for dep in adj_list[task]:\n",
    "            latest_finish[dep] = min(latest_finish[dep], latest_start[task])\n",
    "            latest_start[dep] = latest_finish[dep] - tasks[dep][1]\n",
    "    \n",
    "    return latest_start, latest_finish\n",
    "\n",
    "def find_critical_path(earliest_start, latest_start):\n",
    "    critical_path = []\n",
    "    for task in earliest_start:\n",
    "        if earliest_start[task] == latest_start[task]:\n",
    "            critical_path.append(task)\n",
    "    return critical_path\n",
    "\n",
    "def generate_daily_schedule(tasks, earliest_start, earliest_finish, start_date):\n",
    "    worker_schedule = defaultdict(list)\n",
    "    max_daily_hours = 8\n",
    "    \n",
    "    for task, (worker, duration) in tasks.items():\n",
    "        start = earliest_start[task]\n",
    "        remaining_hours = duration\n",
    "        \n",
    "        current_hour = start\n",
    "        while remaining_hours > 0:\n",
    "            hours_worked = min(remaining_hours, max_daily_hours - (current_hour % max_daily_hours))\n",
    "            day = int((current_hour // max_daily_hours) + 1)  # Start days from 1 instead of 0, convert to int\n",
    "            task_date = start_date + timedelta(days=day-1)  # Convert day to date\n",
    "            worker_schedule[worker].append((task_date, task, int(hours_worked)))  # Ensure hours worked is an integer\n",
    "            remaining_hours -= hours_worked\n",
    "            current_hour += hours_worked\n",
    "    \n",
    "    # Sort the schedule by date in reverse order (earliest dates first)\n",
    "    for worker in worker_schedule:\n",
    "        worker_schedule[worker].sort(key=lambda x: x[0], reverse=False)\n",
    "    \n",
    "    return worker_schedule\n",
    "\n",
    "def critical_path_method(tasks, dependencies, start_date, deadline):\n",
    "    earliest_start, earliest_finish = calculate_earliest_times(tasks, dependencies)\n",
    "    project_duration = int(max(earliest_finish.values()))  # Ensure project duration is an integer\n",
    "    latest_start, latest_finish = calculate_latest_times(tasks, dependencies, project_duration)\n",
    "    critical_path = find_critical_path(earliest_start, latest_start)\n",
    "    worker_schedule = generate_daily_schedule(tasks, earliest_start, earliest_finish, start_date)\n",
    "    \n",
    "    project_end_date = start_date + timedelta(days=(project_duration // 8) - 1)  # Calculate end date based on duration\n",
    "\n",
    "    print(\"Project Deadline:\", deadline.strftime('%d/%m/%y'))\n",
    "    print(\"Project Duration:\", project_duration, \"hours\")\n",
    "    print(\"Project Start Date:\", start_date.strftime('%d/%m/%y'))\n",
    "    print(\"Project End Date:\", project_end_date.strftime('%d/%m/%y'))\n",
    "\n",
    "    if project_end_date <= deadline:\n",
    "        print(\"It is feasible and achievable\")\n",
    "    else:\n",
    "        print(\"Need more resources and time\")\n",
    "    \n",
    "    print(\"\\nWorker Schedules:\")\n",
    "    for worker, schedule in worker_schedule.items():\n",
    "        print(f\"Schedule for {worker}:\")\n",
    "        for date, task, hours in schedule:\n",
    "            print(f\"  {date.strftime('%d/%m/%y')}: {task} ({hours} hours)\")\n",
    "\n",
    "common_dependencies = {\n",
    "    \"Analisis Kebutuhan\": [],\n",
    "    \"Desain UI/UX\": [\"Analisis Kebutuhan\"],\n",
    "    \"Perancangan Basis Data\": [\"Analisis Kebutuhan\"],\n",
    "    \"Pembuatan Basis Data\": [\"Perancangan Basis Data\"],\n",
    "    \"Frontend Development\": [\"Desain UI/UX\"],\n",
    "    \"Backend Development\": [\"Perancangan Basis Data\"],\n",
    "    \"Pengembangan API\": [\"Backend Development\"],\n",
    "    \"Integrasi API\": [\"Pengembangan API\"],\n",
    "    \"Pengujian Unit\": [\"Frontend Development\", \"Backend Development\"],\n",
    "    \"Pengujian Integrasi\": [\"Integrasi API\", \"Frontend Development\", \"Backend Development\"],\n",
    "    \"Integrasi Model\": [\"Integrasi API\"],\n",
    "    \"Pengujian Sistem\": [\"Pengujian Integrasi\"],\n",
    "    \"Pengujian Fungsionalitas\": [\"Pengujian Sistem\"],\n",
    "    \"Pengujian User Acceptance (UAT)\": [\"Pengujian Fungsionalitas\"],\n",
    "    \"Pengujian dan Perbaikan\": [\"Pengujian User Acceptance (UAT)\"],\n",
    "    \"Evaluasi Model\": [\"Integrasi Model\"],\n",
    "    \"Pembersihan dan Preprocessing Data\": [\"Pengumpulan Data\"],\n",
    "    \"Pengumpulan Data\": [],\n",
    "    \"Visualisasi Data\": [\"Pembersihan dan Preprocessing Data\"],\n",
    "    \"Implementasi Fitur\": [\"Frontend Development\", \"Backend Development\"],\n",
    "    \"Dokumentasi\": [\"Implementasi Fitur\", \"Frontend Development\", \"Backend Development\"],\n",
    "    \"Deployment\": [\"Pengujian dan Perbaikan\", \"Frontend Development\", \"Backend Development\", \"Desain UI/UX\"],\n",
    "    \"Presentasi dan Demo\": [\"Deployment\"]\n",
    "}\n",
    "\n",
    "def apply_common_dependencies(predicted_labels):\n",
    "    predicted_labels = [str(label) for label in predicted_labels]\n",
    "    unique_labels = [f\"{label}_{idx}\" for idx, label in enumerate(predicted_labels)]\n",
    "    label_to_unique = dict(zip(predicted_labels, unique_labels))\n",
    "    dependencies = {}\n",
    "    for task, deps in common_dependencies.items():\n",
    "        if task in label_to_unique:\n",
    "            unique_task = label_to_unique[task]\n",
    "            unique_deps = [label_to_unique[dep] for dep in deps if dep in label_to_unique]\n",
    "            dependencies[unique_task] = unique_deps\n",
    "    return dependencies\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "# Example usage\n",
    "new_tasks = [\n",
    "    \"Mengimplementasikan tampilan percakapan yang menarik dan informatif, serta elemen-elemen interaktif seperti tombol dan formulir.\", \n",
    "    \"Membuat sistem moderasi konten untuk platform media sosial.\",\n",
    "    \"Mengamankan akses ke platform cloud dan aplikasi web dengan menerapkan autentikasi dan otorisasi yang sesuai.\",\n",
    "    \"Membuat style guide yang lengkap dengan komponen UI yang dapat digunakan ulang untuk aplikasi mobile perusahaan.\"\n",
    "]\n",
    "\n",
    "task_workers = [\"UserID1\", \"UserID2\", \"UserID1\", \"UserID1\"]\n",
    "\n",
    "predicted_labels = predict_task_labels(loaded_model, tokenizer, label_encoder, new_tasks)\n",
    "tasks = assign_workers_to_tasks(predicted_labels, task_workers)\n",
    "dependencies = apply_common_dependencies(predicted_labels)\n",
    "\n",
    "# Define the start date for the schedule\n",
    "start_date = datetime(2024, 6, 10)\n",
    "deadline = datetime(2024, 12, 12)\n",
    "\n",
    "# Run the CPM algorithm with user inputs\n",
    "critical_path_method(tasks, dependencies, start_date, deadline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b4aa1ed-466a-4236-ae58-e085fcbee0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haidarhanif18/.conda/envs/haidarenv/lib/python3.9/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "2024-06-19 12:40:58.382453: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-19 12:41:00.435702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 58611 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:87:00.0, compute capability: 8.0\n",
      "2024-06-19 12:41:07.637261: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 8s 8s/step\n",
      "['Pengujian User Acceptance (UAT)' 'Backend Development']\n",
      "Project Deadline: 12/12/24\n",
      "Project Duration: 50 hours\n",
      "Project Start Date: 02/01/24\n",
      "Project End Date: 07/01/24\n",
      "It is feasible and achievable\n",
      "\n",
      "Worker Schedules:\n",
      "Schedule for UserID1:\n",
      "  02/01/24: Pengujian User Acceptance (UAT) (8 hours)\n",
      "  03/01/24: Pengujian User Acceptance (UAT) (8 hours)\n",
      "  04/01/24: Pengujian User Acceptance (UAT) (8 hours)\n",
      "  05/01/24: Pengujian User Acceptance (UAT) (8 hours)\n",
      "  06/01/24: Pengujian User Acceptance (UAT) (8 hours)\n",
      "Schedule for UserID2:\n",
      "  02/01/24: Backend Development (8 hours)\n",
      "  03/01/24: Backend Development (8 hours)\n",
      "  04/01/24: Backend Development (8 hours)\n",
      "  05/01/24: Backend Development (8 hours)\n",
      "  06/01/24: Backend Development (8 hours)\n",
      "  07/01/24: Backend Development (8 hours)\n",
      "  08/01/24: Backend Development (2 hours)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 12:41:10.736847: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-06-19 12:41:10.737811: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-06-19 12:41:10.737832: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-06-19 12:41:10.738386: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-06-19 12:41:10.738449: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2024-06-19 12:41:10.832021: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, deque\n",
    "import urllib.request\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Function to download model from URL\n",
    "def download_model_from_url(model_url, save_path):\n",
    "    urllib.request.urlretrieve(model_url, save_path)\n",
    "\n",
    "# URL of the model\n",
    "model_url = \"https://storage.googleapis.com/nexlink-ml-api/text_classify.h5\"\n",
    "model_save_path = \"text_classify.h5\"\n",
    "\n",
    "# Download the model file\n",
    "download_model_from_url(model_url, model_save_path)\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model(model_save_path)\n",
    "\n",
    "# Load tokenizer and label encoder\n",
    "with open('tokenizer.pkl', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('label_encoder.pkl', 'rb') as handle:\n",
    "    label_encoder = pickle.load(handle)\n",
    "\n",
    "# Define your max_length\n",
    "max_length = 100  # Adjust this based on your preprocessing\n",
    "\n",
    "def predict_task_labels(model, tokenizer, label_encoder, tasks):\n",
    "    sequences = tokenizer.texts_to_sequences(tasks)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "    predictions = model.predict(padded_sequences)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    predicted_labels = label_encoder.inverse_transform(predicted_classes)\n",
    "    return predicted_labels\n",
    "\n",
    "def get_task_durations(task, data):\n",
    "    data_choice = data[data['Label_Task'] == task]\n",
    "    if data_choice.empty:\n",
    "        raise ValueError(f\"Task '{task}' not found in the dataset.\")\n",
    "    duration = data_choice['Estimated_hours'].iloc[0]\n",
    "    return duration\n",
    "\n",
    "def assign_workers_to_tasks(task_labels, task_workers, data):\n",
    "    worker_assignments = defaultdict(list)\n",
    "    for task, worker in zip(task_labels, task_workers):\n",
    "        task_name = re.split(r'_\\d+', task)[0]\n",
    "        duration = get_task_durations(task_name, data)\n",
    "        worker_assignments[worker].append((task_name, duration))\n",
    "    return worker_assignments\n",
    "\n",
    "def calculate_earliest_times(tasks, dependencies):\n",
    "    earliest_start = {task: 0 for task in tasks}\n",
    "    earliest_finish = {task: duration for task, duration in tasks.items()}\n",
    "    \n",
    "    adj_list = defaultdict(list)\n",
    "    in_degree = {task: 0 for task in tasks}\n",
    "    \n",
    "    for task, deps in dependencies.items():\n",
    "        for dep in deps:\n",
    "            adj_list[dep].append(task)\n",
    "            in_degree[task] += 1\n",
    "    \n",
    "    topo_order = []\n",
    "    zero_in_degree_queue = deque([task for task in tasks if in_degree[task] == 0])\n",
    "    \n",
    "    while zero_in_degree_queue:\n",
    "        task = zero_in_degree_queue.popleft()\n",
    "        topo_order.append(task)\n",
    "        \n",
    "        for neighbor in adj_list[task]:\n",
    "            in_degree[neighbor] -= 1\n",
    "            if in_degree[neighbor] == 0:\n",
    "                zero_in_degree_queue.append(neighbor)\n",
    "    \n",
    "    for task in topo_order:\n",
    "        for neighbor in adj_list[task]:\n",
    "            earliest_start[neighbor] = max(earliest_start[neighbor], earliest_finish[task])\n",
    "            earliest_finish[neighbor] = earliest_start[neighbor] + tasks[neighbor]\n",
    "    \n",
    "    return earliest_start, earliest_finish\n",
    "\n",
    "def calculate_latest_times(tasks, dependencies, project_duration):\n",
    "    latest_finish = {task: project_duration for task in tasks}\n",
    "    latest_start = {task: project_duration - duration for task, duration in tasks.items()}\n",
    "    \n",
    "    adj_list = defaultdict(list)\n",
    "    for task, deps in dependencies.items():\n",
    "        for dep in deps:\n",
    "            adj_list[task].append(dep)\n",
    "    \n",
    "    for task in reversed(list(tasks.keys())):\n",
    "        for dep in adj_list[task]:\n",
    "            latest_finish[dep] = min(latest_finish[dep], latest_start[task])\n",
    "            latest_start[dep] = latest_finish[dep] - tasks[dep]\n",
    "    \n",
    "    return latest_start, latest_finish\n",
    "\n",
    "def find_critical_path(earliest_start, latest_start):\n",
    "    critical_path = []\n",
    "    for task in earliest_start:\n",
    "        if earliest_start[task] == latest_start[task]:\n",
    "            critical_path.append(task)\n",
    "    return critical_path\n",
    "\n",
    "def generate_daily_schedule(tasks, earliest_start, earliest_finish, start_date):\n",
    "    worker_schedule = defaultdict(list)\n",
    "    max_daily_hours = 8\n",
    "    \n",
    "    for worker, worker_tasks in tasks.items():\n",
    "        current_hour = 0\n",
    "        for task, duration in worker_tasks:\n",
    "            remaining_hours = duration\n",
    "            while remaining_hours > 0:\n",
    "                hours_worked = min(remaining_hours, max_daily_hours - (current_hour % max_daily_hours))\n",
    "                day = int((current_hour // max_daily_hours) + 1)  # Start days from 1 instead of 0, convert to int\n",
    "                task_date = start_date + timedelta(days=day-1)  # Convert day to date\n",
    "                worker_schedule[worker].append((task_date, task, int(hours_worked)))  # Ensure hours worked is an integer\n",
    "                remaining_hours -= hours_worked\n",
    "                current_hour += hours_worked\n",
    "    \n",
    "    # Sort the schedule by date in reverse order (earliest dates first)\n",
    "    for worker in worker_schedule:\n",
    "        worker_schedule[worker].sort(key=lambda x: x[0], reverse=False)\n",
    "    \n",
    "    return worker_schedule\n",
    "\n",
    "def critical_path_method(tasks, dependencies, start_date, deadline):\n",
    "    task_durations = {task: sum(duration for _, duration in worker_tasks) for worker_tasks in tasks.values() for task, duration in worker_tasks}\n",
    "    \n",
    "    # Check for NaN values and handle them\n",
    "    if any(pd.isna(duration) for duration in task_durations.values()):\n",
    "        raise ValueError(\"Some tasks have NaN durations. Please check the dataset.\")\n",
    "    \n",
    "    earliest_start, earliest_finish = calculate_earliest_times(task_durations, dependencies)\n",
    "    project_duration = int(max(earliest_finish.values()))  # Ensure project duration is an integer\n",
    "    latest_start, latest_finish = calculate_latest_times(task_durations, dependencies, project_duration)\n",
    "    critical_path = find_critical_path(earliest_start, latest_start)\n",
    "    worker_schedule = generate_daily_schedule(tasks, earliest_start, earliest_finish, start_date)\n",
    "    \n",
    "    project_end_date = start_date + timedelta(days=(project_duration // 8) - 1)  # Calculate end date based on duration\n",
    "\n",
    "    print(\"Project Deadline:\", deadline.strftime('%d/%m/%y'))\n",
    "    print(\"Project Duration:\", project_duration, \"hours\")\n",
    "    print(\"Project Start Date:\", start_date.strftime('%d/%m/%y'))\n",
    "    print(\"Project End Date:\", project_end_date.strftime('%d/%m/%y'))\n",
    "\n",
    "    if project_end_date <= deadline:\n",
    "        print(\"It is feasible and achievable\")\n",
    "    else:\n",
    "        print(\"Need more resources and time\")\n",
    "    \n",
    "    print(\"\\nWorker Schedules:\")\n",
    "    for worker, schedule in worker_schedule.items():\n",
    "        print(f\"Schedule for {worker}:\")\n",
    "        for date, task, hours in schedule:\n",
    "            print(f\"  {date.strftime('%d/%m/%y')}: {task} ({hours} hours)\")\n",
    "\n",
    "common_dependencies = {\n",
    "    \"Analisis Kebutuhan\": [],\n",
    "    \"Desain UI/UX\": [\"Analisis Kebutuhan\"],\n",
    "    \"Perancangan Basis Data\": [\"Analisis Kebutuhan\"],\n",
    "    \"Pembuatan Basis Data\": [\"Perancangan Basis Data\"],\n",
    "    \"Frontend Development\": [\"Desain UI/UX\"],\n",
    "    \"Backend Development\": [\"Perancangan Basis Data\"],\n",
    "    \"Pengembangan API\": [\"Backend Development\"],\n",
    "    \"Integrasi API\": [\"Pengembangan API\"],\n",
    "    \"Pengujian Unit\": [\"Frontend Development\", \"Backend Development\"],\n",
    "    \"Pengujian Integrasi\": [\"Integrasi API\", \"Frontend Development\", \"Backend Development\"],\n",
    "    \"Integrasi Model\": [\"Integrasi API\"],\n",
    "    \"Pengujian Sistem\": [\"Pengujian Integrasi\"],\n",
    "    \"Pengujian Fungsionalitas\": [\"Pengujian Sistem\"],\n",
    "    \"Pengujian User Acceptance (UAT)\": [\"Pengujian Fungsionalitas\"],\n",
    "    \"Pengujian dan Perbaikan\": [\"Pengujian User Acceptance (UAT)\"],\n",
    "    \"Evaluasi Model\": [\"Integrasi Model\"],\n",
    "    \"Pembersihan dan Preprocessing Data\": [\"Pengumpulan Data\"],\n",
    "    \"Pengumpulan Data\": [],\n",
    "    \"Visualisasi Data\": [\"Pembersihan dan Preprocessing Data\"],\n",
    "    \"Implementasi Fitur\": [\"Frontend Development\", \"Backend Development\"],\n",
    "    \"Dokumentasi\": [\"Implementasi Fitur\", \"Frontend Development\", \"Backend Development\"],\n",
    "    \"Deployment\": [\"Pengujian dan Perbaikan\", \"Frontend Development\", \"Backend Development\", \"Desain UI/UX\"],\n",
    "    \"Presentasi dan Demo\": [\"Deployment\"]\n",
    "}\n",
    "\n",
    "def apply_common_dependencies(predicted_labels):\n",
    "    predicted_labels = [str(label) for label in predicted_labels]\n",
    "    unique_labels = [re.split(r'_\\d+', label)[0] for label in predicted_labels]\n",
    "    label_to_unique = dict(zip(predicted_labels, unique_labels))\n",
    "    dependencies = {}\n",
    "    for task, deps in common_dependencies.items():\n",
    "        if task in label_to_unique:\n",
    "            unique_task = label_to_unique[task]\n",
    "            unique_deps = [label_to_unique[dep] for dep in deps if dep in label_to_unique]\n",
    "            dependencies[unique_task] = unique_deps\n",
    "    return dependencies\n",
    "\n",
    "# Example usage\n",
    "new_tasks = [\n",
    "    \"Pengujian User Acceptance (UAT)\", \n",
    "    \"Backend Development\"\n",
    "]\n",
    "\n",
    "task_workers = [\"UserID1\", \"UserID2\"]\n",
    "\n",
    "# Load your dataset for task durations\n",
    "data = pd.read_csv('dataset5.csv')\n",
    "\n",
    "predicted_labels = predict_task_labels(loaded_model, tokenizer, label_encoder, new_tasks)\n",
    "\n",
    "print(predicted_labels)\n",
    "tasks = assign_workers_to_tasks(predicted_labels, task_workers, data)\n",
    "dependencies = apply_common_dependencies(predicted_labels)\n",
    "\n",
    "# Define the start date for the schedule 2024-01-02\n",
    "start_date = datetime(2024, 1, 2)\n",
    "deadline = datetime(2024, 12, 12)\n",
    "\n",
    "# Run the CPM algorithm with user inputs\n",
    "critical_path_method(tasks, dependencies, start_date, deadline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dadbb142-6ee9-4721-8603-7f6316b51b02",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flask, request, jsonify\n\u001b[1;32m     14\u001b[0m app \u001b[38;5;241m=\u001b[39m Flask(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Function to download model from URL\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flask'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, deque\n",
    "import urllib.request\n",
    "import re\n",
    "import pickle\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Function to download model from URL\n",
    "def download_model_from_url(model_url, save_path):\n",
    "    urllib.request.urlretrieve(model_url, save_path)\n",
    "\n",
    "# URL of the model\n",
    "model_url = \"https://storage.googleapis.com/nexlink-ml-api/text_classify.h5\"\n",
    "model_save_path = \"text_classify.h5\"\n",
    "\n",
    "# Download the model file\n",
    "download_model_from_url(model_url, model_save_path)\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model(model_save_path)\n",
    "\n",
    "# Load tokenizer and label encoder\n",
    "with open('tokenizer.pkl', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('label_encoder.pkl', 'rb') as handle:\n",
    "    label_encoder = pickle.load(handle)\n",
    "\n",
    "# Define your max_length\n",
    "max_length = 100  # Adjust this based on your preprocessing\n",
    "\n",
    "def predict_task_labels(model, tokenizer, label_encoder, tasks):\n",
    "    sequences = tokenizer.texts_to_sequences(tasks)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "    predictions = model.predict(padded_sequences)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    predicted_labels = label_encoder.inverse_transform(predicted_classes)\n",
    "    return predicted_labels\n",
    "\n",
    "def get_task_durations(task, data):\n",
    "    data_choice = data[data['Label_Task'] == task]\n",
    "    if data_choice.empty:\n",
    "        raise ValueError(f\"Task '{task}' not found in the dataset.\")\n",
    "    duration = data_choice['Estimated_hours'].iloc[0]\n",
    "    return duration\n",
    "\n",
    "def assign_workers_to_tasks(task_labels, task_workers, data):\n",
    "    worker_assignments = defaultdict(list)\n",
    "    for task, worker in zip(task_labels, task_workers):\n",
    "        task_name = re.split(r'_\\d+', task)[0]\n",
    "        duration = get_task_durations(task_name, data)\n",
    "        worker_assignments[worker].append((task_name, duration))\n",
    "    return worker_assignments\n",
    "\n",
    "def calculate_earliest_times(tasks, dependencies):\n",
    "    earliest_start = {task: 0 for task in tasks}\n",
    "    earliest_finish = {task: duration for task, duration in tasks.items()}\n",
    "    \n",
    "    adj_list = defaultdict(list)\n",
    "    in_degree = {task: 0 for task in tasks}\n",
    "    \n",
    "    for task, deps in dependencies.items():\n",
    "        for dep in deps:\n",
    "            adj_list[dep].append(task)\n",
    "            in_degree[task] += 1\n",
    "    \n",
    "    topo_order = []\n",
    "    zero_in_degree_queue = deque([task for task in tasks if in_degree[task] == 0])\n",
    "    \n",
    "    while zero_in_degree_queue:\n",
    "        task = zero_in_degree_queue.popleft()\n",
    "        topo_order.append(task)\n",
    "        \n",
    "        for neighbor in adj_list[task]:\n",
    "            in_degree[neighbor] -= 1\n",
    "            if in_degree[neighbor] == 0:\n",
    "                zero_in_degree_queue.append(neighbor)\n",
    "    \n",
    "    for task in topo_order:\n",
    "        for neighbor in adj_list[task]:\n",
    "            earliest_start[neighbor] = max(earliest_start[neighbor], earliest_finish[task])\n",
    "            earliest_finish[neighbor] = earliest_start[neighbor] + tasks[neighbor]\n",
    "    \n",
    "    return earliest_start, earliest_finish\n",
    "\n",
    "def calculate_latest_times(tasks, dependencies, project_duration):\n",
    "    latest_finish = {task: project_duration for task in tasks}\n",
    "    latest_start = {task: project_duration - duration for task, duration in tasks.items()}\n",
    "    \n",
    "    adj_list = defaultdict(list)\n",
    "    for task, deps in dependencies.items():\n",
    "        for dep in deps:\n",
    "            adj_list[task].append(dep)\n",
    "    \n",
    "    for task in reversed(list(tasks.keys())):\n",
    "        for dep in adj_list[task]:\n",
    "            latest_finish[dep] = min(latest_finish[dep], latest_start[task])\n",
    "            latest_start[dep] = latest_finish[dep] - tasks[dep]\n",
    "    \n",
    "    return latest_start, latest_finish\n",
    "\n",
    "def find_critical_path(earliest_start, latest_start):\n",
    "    critical_path = []\n",
    "    for task in earliest_start:\n",
    "        if earliest_start[task] == latest_start[task]:\n",
    "            critical_path.append(task)\n",
    "    return critical_path\n",
    "\n",
    "def generate_daily_schedule(tasks, earliest_start, earliest_finish, start_date):\n",
    "    worker_schedule = defaultdict(list)\n",
    "    max_daily_hours = 8\n",
    "    \n",
    "    for worker, worker_tasks in tasks.items():\n",
    "        current_hour = 0\n",
    "        for task, duration in worker_tasks:\n",
    "            remaining_hours = duration\n",
    "            while remaining_hours > 0:\n",
    "                hours_worked = min(remaining_hours, max_daily_hours - (current_hour % max_daily_hours))\n",
    "                day = int((current_hour // max_daily_hours) + 1)  # Start days from 1 instead of 0, convert to int\n",
    "                task_date = start_date + timedelta(days=day-1)  # Convert day to date\n",
    "                worker_schedule[worker].append((task_date, task, int(hours_worked)))  # Ensure hours worked is an integer\n",
    "                remaining_hours -= hours_worked\n",
    "                current_hour += hours_worked\n",
    "    \n",
    "    # Sort the schedule by date in reverse order (earliest dates first)\n",
    "    for worker in worker_schedule:\n",
    "        worker_schedule[worker].sort(key=lambda x: x[0], reverse=False)\n",
    "    \n",
    "    return worker_schedule\n",
    "\n",
    "def critical_path_method(tasks, dependencies, start_date, deadline):\n",
    "    task_durations = {task: sum(duration for _, duration in worker_tasks) for worker_tasks in tasks.values() for task, duration in worker_tasks}\n",
    "    \n",
    "    # Check for NaN values and handle them\n",
    "    if any(pd.isna(duration) for duration in task_durations.values()):\n",
    "        raise ValueError(\"Some tasks have NaN durations. Please check the dataset.\")\n",
    "    \n",
    "    earliest_start, earliest_finish = calculate_earliest_times(task_durations, dependencies)\n",
    "    project_duration = int(max(earliest_finish.values()))  # Ensure project duration is an integer\n",
    "    latest_start, latest_finish = calculate_latest_times(task_durations, dependencies, project_duration)\n",
    "    critical_path = find_critical_path(earliest_start, latest_start)\n",
    "    worker_schedule = generate_daily_schedule(tasks, earliest_start, earliest_finish, start_date)\n",
    "    \n",
    "    project_end_date = start_date + timedelta(days=(project_duration // 8) - 1)  # Calculate end date based on duration\n",
    "\n",
    "    response = {\n",
    "        \"project_deadline\": deadline.strftime('%d/%m/%y'),\n",
    "        \"project_duration\": project_duration,\n",
    "        \"project_start_date\": start_date.strftime('%d/%m/%y'),\n",
    "        \"project_end_date\": project_end_date.strftime('%d/%m/%y'),\n",
    "        \"feasible\": project_end_date <= deadline,\n",
    "        \"worker_schedules\": {}\n",
    "    }\n",
    "\n",
    "    for worker, schedule in worker_schedule.items():\n",
    "        response[\"worker_schedules\"][worker] = [\n",
    "            {\"date\": date.strftime('%d/%m/%y'), \"task\": task, \"hours\": hours} for date, task, hours in schedule\n",
    "        ]\n",
    "\n",
    "    return response\n",
    "\n",
    "common_dependencies = {\n",
    "    \"Analisis Kebutuhan\": [],\n",
    "    \"Desain UI/UX\": [\"Analisis Kebutuhan\"],\n",
    "    \"Perancangan Basis Data\": [\"Analisis Kebutuhan\"],\n",
    "    \"Pembuatan Basis Data\": [\"Perancangan Basis Data\"],\n",
    "    \"Frontend Development\": [\"Desain UI/UX\"],\n",
    "    \"Backend Development\": [\"Perancangan Basis Data\"],\n",
    "    \"Pengembangan API\": [\"Backend Development\"],\n",
    "    \"Integrasi API\": [\"Pengembangan API\"],\n",
    "    \"Pengujian Unit\": [\"Frontend Development\", \"Backend Development\"],\n",
    "    \"Pengujian Integrasi\": [\"Integrasi API\", \"Frontend Development\", \"Backend Development\"],\n",
    "    \"Integrasi Model\": [\"Integrasi API\"],\n",
    "    \"Pengujian Sistem\": [\"Pengujian Integrasi\"],\n",
    "    \"Pengujian Fungsionalitas\": [\"Pengujian Sistem\"],\n",
    "    \"Pengujian User Acceptance (UAT)\": [\"Pengujian Fungsionalitas\"],\n",
    "    \"Pengujian dan Perbaikan\": [\"Pengujian User Acceptance (UAT)\"],\n",
    "    \"Evaluasi Model\": [\"Integrasi Model\"],\n",
    "    \"Pembersihan dan Preprocessing Data\": [\"Pengumpulan Data\"],\n",
    "    \"Pengumpulan Data\": [],\n",
    "    \"Visualisasi Data\": [\"Pembersihan dan Preprocessing Data\"],\n",
    "    \"Implementasi Fitur\": [\"Frontend Development\", \"Backend Development\"],\n",
    "    \"Dokumentasi\": [\"Implementasi Fitur\", \"Frontend Development\", \"Backend Development\"],\n",
    "    \"Deployment\": [\"Pengujian dan Perbaikan\", \"Frontend Development\", \"Backend Development\", \"Desain UI/UX\"],\n",
    "    \"Presentasi dan Demo\": [\"Deployment\"]\n",
    "}\n",
    "\n",
    "def apply_common_dependencies(predicted_labels):\n",
    "    predicted_labels = [str(label) for label in predicted_labels]\n",
    "    unique_labels = [re.split(r'_\\d+', label)[0] for label in predicted_labels]\n",
    "    label_to_unique = dict(zip(predicted_labels, unique_labels))\n",
    "    dependencies = {}\n",
    "    for task, deps in common_dependencies.items():\n",
    "        if task in label_to_unique:\n",
    "            unique_task = label_to_unique[task]\n",
    "            unique_deps = [label_to_unique[dep] for dep in deps if dep in label_to_unique]\n",
    "            dependencies[unique_task] = unique_deps\n",
    "    return dependencies\n",
    "\n",
    "@app.route('/schedule', methods=['POST'])\n",
    "def schedule():\n",
    "    data = request.get_json()\n",
    "    new_tasks = data['new_tasks']\n",
    "    task_workers = data['task_workers']\n",
    "    start_date = datetime.strptime(data['start_date'], '%Y-%m-%d')\n",
    "    deadline = datetime.strptime(data['deadline'], '%Y-%m-%d')\n",
    "    \n",
    "    # Load your dataset for task durations\n",
    "    task_data = pd.read_csv('dataset5.csv')\n",
    "    \n",
    "    predicted_labels = predict_task_labels(loaded_model, tokenizer, label_encoder, new_tasks)\n",
    "    tasks = assign_workers_to_tasks(predicted_labels, task_workers, task_data)\n",
    "    dependencies = apply_common_dependencies(predicted_labels)\n",
    "    \n",
    "    response = critical_path_method(tasks, dependencies, start_date, deadline)\n",
    "    \n",
    "    return jsonify(response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=8080)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9(haidarenv)",
   "language": "python",
   "name": "haidarenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
