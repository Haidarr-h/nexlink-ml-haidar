{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b37bfd-4ae3-49bd-b02f-154fc1e80d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, deque\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8288742-4143-41c6-8d98-5d7abc00dbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print('gpu ', gpu)\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92eb5138-e0a7-4b4f-8e7f-8b0de6ed32e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 12:41:11.705143: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-13 12:41:14.696445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21907 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:87:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "loaded_model = load_model(\"text_classify.h5\")\n",
    "\n",
    "# Load tokenizer and label encoder\n",
    "with open('tokenizer.pkl', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('label_encoder.pkl', 'rb') as handle:\n",
    "    label_encoder = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7106d368-ce1e-4d0c-8dbd-6abeddc44635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your max_length\n",
    "max_length = 100  # Adjust this based on your preprocessing\n",
    "\n",
    "def predict_task_labels(model, tokenizer, label_encoder, tasks):\n",
    "    sequences = tokenizer.texts_to_sequences(tasks)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "    predictions = model.predict(padded_sequences)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    predicted_labels = label_encoder.inverse_transform(predicted_classes)\n",
    "    return predicted_labels\n",
    "\n",
    "def get_task_durations(task):\n",
    "    data = pd.read_csv('dataset5.csv')\n",
    "    data_choice = data[data['Label_Task'] == task]\n",
    "    if data_choice.empty:\n",
    "        raise ValueError(f\"Task '{task}' not found in the dataset.\")\n",
    "    duration = data_choice['Estimated_hours'].iloc[0]\n",
    "    return duration\n",
    "\n",
    "def assign_workers_to_tasks(task_labels, task_workers):\n",
    "    worker_assignments = {}\n",
    "    for idx, (task, worker) in enumerate(zip(task_labels, task_workers)):\n",
    "        unique_task = f\"{task}_{idx}\"  # Ensure each task is unique\n",
    "        duration = get_task_durations(task)\n",
    "        worker_assignments[unique_task] = (worker, duration)\n",
    "    return worker_assignments\n",
    "\n",
    "def calculate_earliest_times(tasks, dependencies):\n",
    "    earliest_start = {task: 0 for task in tasks}\n",
    "    earliest_finish = {task: duration for task, (worker, duration) in tasks.items()}\n",
    "    \n",
    "    adj_list = defaultdict(list)\n",
    "    in_degree = {task: 0 for task in tasks}\n",
    "    \n",
    "    for task, deps in dependencies.items():\n",
    "        for dep in deps:\n",
    "            adj_list[dep].append(task)\n",
    "            in_degree[task] += 1\n",
    "    \n",
    "    topo_order = []\n",
    "    zero_in_degree_queue = deque([task for task in tasks if in_degree[task] == 0])\n",
    "    \n",
    "    while zero_in_degree_queue:\n",
    "        task = zero_in_degree_queue.popleft()\n",
    "        topo_order.append(task)\n",
    "        \n",
    "        for neighbor in adj_list[task]:\n",
    "            in_degree[neighbor] -= 1\n",
    "            if in_degree[neighbor] == 0:\n",
    "                zero_in_degree_queue.append(neighbor)\n",
    "    \n",
    "    for task in topo_order:\n",
    "        for neighbor in adj_list[task]:\n",
    "            earliest_start[neighbor] = max(earliest_start[neighbor], earliest_finish[task])\n",
    "            earliest_finish[neighbor] = earliest_start[neighbor] + tasks[neighbor][1]\n",
    "    \n",
    "    return earliest_start, earliest_finish\n",
    "\n",
    "def calculate_latest_times(tasks, dependencies, project_duration):\n",
    "    latest_finish = {task: project_duration for task in tasks}\n",
    "    latest_start = {task: project_duration - duration for task, (worker, duration) in tasks.items()}\n",
    "    \n",
    "    adj_list = defaultdict(list)\n",
    "    for task, deps in dependencies.items():\n",
    "        for dep in deps:\n",
    "            adj_list[task].append(dep)\n",
    "    \n",
    "    for task in reversed(list(tasks.keys())):\n",
    "        for dep in adj_list[task]:\n",
    "            latest_finish[dep] = min(latest_finish[dep], latest_start[task])\n",
    "            latest_start[dep] = latest_finish[dep] - tasks[dep][1]\n",
    "    \n",
    "    return latest_start, latest_finish\n",
    "\n",
    "def find_critical_path(earliest_start, latest_start):\n",
    "    critical_path = []\n",
    "    for task in earliest_start:\n",
    "        if earliest_start[task] == latest_start[task]:\n",
    "            critical_path.append(task)\n",
    "    return critical_path\n",
    "\n",
    "def generate_daily_schedule(tasks, earliest_start, earliest_finish, start_date):\n",
    "    worker_schedule = defaultdict(list)\n",
    "    max_daily_hours = 8\n",
    "    \n",
    "    for task, (worker, duration) in tasks.items():\n",
    "        start = earliest_start[task]\n",
    "        remaining_hours = duration\n",
    "        \n",
    "        current_hour = start\n",
    "        while remaining_hours > 0:\n",
    "            hours_worked = min(remaining_hours, max_daily_hours - (current_hour % max_daily_hours))\n",
    "            day = int((current_hour // max_daily_hours) + 1)  # Start days from 1 instead of 0, convert to int\n",
    "            task_date = start_date + timedelta(days=day-1)  # Convert day to date\n",
    "            worker_schedule[worker].append((task_date, task, int(hours_worked)))  # Ensure hours worked is an integer\n",
    "            remaining_hours -= hours_worked\n",
    "            current_hour += hours_worked\n",
    "    \n",
    "    # Sort the schedule by date in reverse order (earliest dates first)\n",
    "    for worker in worker_schedule:\n",
    "        worker_schedule[worker].sort(key=lambda x: x[0], reverse=False)\n",
    "    \n",
    "    return worker_schedule\n",
    "\n",
    "def critical_path_method(tasks, dependencies, start_date, deadline):\n",
    "    earliest_start, earliest_finish = calculate_earliest_times(tasks, dependencies)\n",
    "    project_duration = int(max(earliest_finish.values()))  # Ensure project duration is an integer\n",
    "    latest_start, latest_finish = calculate_latest_times(tasks, dependencies, project_duration)\n",
    "    critical_path = find_critical_path(earliest_start, latest_start)\n",
    "    worker_schedule = generate_daily_schedule(tasks, earliest_start, earliest_finish, start_date)\n",
    "    \n",
    "    project_end_date = start_date + timedelta(days=(project_duration // 8) - 1)  # Calculate end date based on duration\n",
    "\n",
    "    print(\"Project Deadline:\", deadline.strftime('%d/%m/%y'))\n",
    "    print(\"Project Duration:\", project_duration, \"hours\")\n",
    "    print(\"Project Start Date:\", start_date.strftime('%d/%m/%y'))\n",
    "    print(\"Project End Date:\", project_end_date.strftime('%d/%m/%y'))\n",
    "\n",
    "    if project_end_date <= deadline:\n",
    "        print(\"It is feasible and achievable\")\n",
    "    else:\n",
    "        print(\"Need more resources and time\")\n",
    "    \n",
    "    print(\"\\nWorker Schedules:\")\n",
    "    for worker, schedule in worker_schedule.items():\n",
    "        print(f\"Schedule for {worker}:\")\n",
    "        for date, task, hours in schedule:\n",
    "            print(f\"  {date.strftime('%d/%m/%y')}: {task} ({hours} hours)\")\n",
    "\n",
    "common_dependencies = {\n",
    "    \"Analisis Kebutuhan\": [],\n",
    "    \"Desain UI/UX\": [\"Analisis Kebutuhan\"],\n",
    "    \"Perancangan Basis Data\": [\"Analisis Kebutuhan\"],\n",
    "    \"Pembuatan Basis Data\": [\"Perancangan Basis Data\"],\n",
    "    \"Frontend Development\": [\"Desain UI/UX\"],\n",
    "    \"Backend Development\": [\"Perancangan Basis Data\"],\n",
    "    \"Pengembangan API\": [\"Backend Development\"],\n",
    "    \"Integrasi API\": [\"Pengembangan API\"],\n",
    "    \"Pengujian Unit\": [\"Frontend Development\", \"Backend Development\"],\n",
    "    \"Pengujian Integrasi\": [\"Integrasi API\", \"Frontend Development\", \"Backend Development\"],\n",
    "    \"Integrasi Model\": [\"Integrasi API\"],\n",
    "    \"Pengujian Sistem\": [\"Pengujian Integrasi\"],\n",
    "    \"Pengujian Fungsionalitas\": [\"Pengujian Sistem\"],\n",
    "    \"Pengujian User Acceptance (UAT)\": [\"Pengujian Fungsionalitas\"],\n",
    "    \"Pengujian dan Perbaikan\": [\"Pengujian User Acceptance (UAT)\"],\n",
    "    \"Evaluasi Model\": [\"Integrasi Model\"],\n",
    "    \"Pembersihan dan Preprocessing Data\": [\"Pengumpulan Data\"],\n",
    "    \"Pengumpulan Data\": [],\n",
    "    \"Visualisasi Data\": [\"Pembersihan dan Preprocessing Data\"],\n",
    "    \"Implementasi Fitur\": [\"Frontend Development\", \"Backend Development\"],\n",
    "    \"Dokumentasi\": [\"Implementasi Fitur\", \"Frontend Development\", \"Backend Development\"],\n",
    "    \"Deployment\": [\"Pengujian dan Perbaikan\", \"Frontend Development\", \"Backend Development\", \"Desain UI/UX\"],\n",
    "    \"Presentasi dan Demo\": [\"Deployment\"]\n",
    "}\n",
    "\n",
    "def apply_common_dependencies(predicted_labels):\n",
    "    predicted_labels = [str(label) for label in predicted_labels]\n",
    "    unique_labels = [f\"{label}_{idx}\" for idx, label in enumerate(predicted_labels)]\n",
    "    label_to_unique = dict(zip(predicted_labels, unique_labels))\n",
    "    dependencies = {}\n",
    "    for task, deps in common_dependencies.items():\n",
    "        if task in label_to_unique:\n",
    "            unique_task = label_to_unique[task]\n",
    "            unique_deps = [label_to_unique[dep] for dep in deps if dep in label_to_unique]\n",
    "            dependencies[unique_task] = unique_deps\n",
    "    return dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a8e65a9-6d7a-42fd-a815-867312903a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n",
      "Project Deadline: 12/12/24\n",
      "Project Duration: 35 hours\n",
      "Project Start Date: 10/06/24\n",
      "Project End Date: 13/06/24\n",
      "It is feasible and achievable\n",
      "\n",
      "Worker Schedules:\n",
      "Schedule for UserID1:\n",
      "  10/06/24: Implementasi Fitur_0 (8 hours)\n",
      "  10/06/24: Pengujian dan Perbaikan_2 (8 hours)\n",
      "  11/06/24: Implementasi Fitur_0 (8 hours)\n",
      "  11/06/24: Pengujian dan Perbaikan_2 (8 hours)\n",
      "  12/06/24: Implementasi Fitur_0 (8 hours)\n",
      "  12/06/24: Pengujian dan Perbaikan_2 (8 hours)\n",
      "  13/06/24: Implementasi Fitur_0 (1 hours)\n",
      "  13/06/24: Pengujian dan Perbaikan_2 (8 hours)\n",
      "  14/06/24: Pengujian dan Perbaikan_2 (3 hours)\n",
      "Schedule for UserID2:\n",
      "  10/06/24: Implementasi Fitur_1 (8 hours)\n",
      "  11/06/24: Implementasi Fitur_1 (8 hours)\n",
      "  12/06/24: Implementasi Fitur_1 (8 hours)\n",
      "  13/06/24: Implementasi Fitur_1 (1 hours)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "new_tasks = [\n",
    "    \"Mengimplementasikan tampilan percakapan yang menarik dan informatif, serta elemen-elemen interaktif seperti tombol dan formulir.\", \n",
    "    \"Membuat sistem moderasi konten untuk platform media sosial.\",\n",
    "    \"Mengamankan akses ke platform cloud dan aplikasi web dengan menerapkan autentikasi dan otorisasi yang sesuai.\",\n",
    "    \"Membuat style guide yang lengkap dengan komponen UI yang dapat digunakan ulang untuk aplikasi mobile perusahaan.\"\n",
    "]\n",
    "\n",
    "task_workers = [\"UserID1\", \"UserID2\", \"UserID1\", \"UserID1\"]\n",
    "\n",
    "predicted_labels = predict_task_labels(loaded_model, tokenizer, label_encoder, new_tasks)\n",
    "tasks = assign_workers_to_tasks(predicted_labels, task_workers)\n",
    "dependencies = apply_common_dependencies(predicted_labels)\n",
    "\n",
    "# Define the start date for the schedule\n",
    "start_date = datetime(2024, 6, 10)\n",
    "deadline = datetime(2024, 12, 12)\n",
    "\n",
    "# Run the CPM algorithm with user inputs\n",
    "critical_path_method(tasks, dependencies, start_date, deadline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9(haidarenv)",
   "language": "python",
   "name": "haidarenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
