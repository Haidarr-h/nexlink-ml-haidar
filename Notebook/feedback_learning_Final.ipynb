{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7ba2a1-5d4a-4e8c-aa57-4c834261f139",
   "metadata": {},
   "source": [
    "## 1. Importing libraries and Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b852e31-81fd-4b86-8614-79873bf98658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haidarhanif18/.conda/envs/haidarenv/lib/python3.9/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, deque\n",
    "import pickle\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6d04afb-6ccc-499c-8880-73e185ee02a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print('gpu ', gpu)\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c7ce3-faae-4810-9b13-f0cb86887f16",
   "metadata": {},
   "source": [
    "## 2. Database Connection and Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96c0c0e2-e71b-457c-8d52-9a4ca3e640da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database\n"
     ]
    }
   ],
   "source": [
    "# Define the connection parameters\n",
    "db_params = {\n",
    "    'dbname': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'AZX)$aPNH4zpDrj}',\n",
    "    'host': '34.173.222.240',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "# Connect to PostgreSQL database\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    print(\"Connected to the database\")\n",
    "except Exception as e:\n",
    "    print(f\"Unable to connect to the database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ec8a132-4547-4bb2-adc8-0433abd80b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_686863/2806474399.py:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>label_task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>buat web front end</td>\n",
       "      <td>Frontend Development\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bikin frontend</td>\n",
       "      <td>Frontend Development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>buat front end website</td>\n",
       "      <td>Frontend Development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bikin front end bagus</td>\n",
       "      <td>Frontend Development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>front end</td>\n",
       "      <td>Frontend Development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>website front end</td>\n",
       "      <td>Frontend Development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>front website</td>\n",
       "      <td>Frontend Development</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                sentences              label_task\n",
       "0      buat web front end  Frontend Development\\n\n",
       "1          bikin frontend    Frontend Development\n",
       "2  buat front end website    Frontend Development\n",
       "3   bikin front end bagus    Frontend Development\n",
       "4               front end    Frontend Development\n",
       "5       website front end    Frontend Development\n",
       "6           front website    Frontend Development"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"SELECT sentences, label_task FROM tbl1;\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f84caf82-105f-4c06-8071-628fc70c0410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed\n"
     ]
    }
   ],
   "source": [
    "conn.close()\n",
    "print(\"Connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9d5ea9f-20b2-46b0-a568-9d533f457930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter labels with at least 45 sentences\n",
    "label_counts = df['label_task'].value_counts()\n",
    "valid_labels = label_counts[label_counts >= 40].index\n",
    "\n",
    "filtered_df = df[df['label_task'].isin(valid_labels)]\n",
    "num_classes = filtered_df['label_task'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb0532e-eead-4167-90bb-bb6ecb24a10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "label_counts = filtered_df['label_task'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "211009ce-3a5d-4069-bfec-b908c7278210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('text_classify.h5')\n",
    "\n",
    "# Load the saved tokenizer and label encoder\n",
    "with open('tokenizer.pkl', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('label_encoder.pkl', 'rb') as handle:\n",
    "    label_encoder = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cc0e855-f83a-4b1d-a41d-aafc868c2f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and tokenize the new data\n",
    "new_sentences = filtered_df['sentences'].tolist()\n",
    "new_labels = filtered_df['label_task'].tolist()\n",
    "\n",
    "new_sequences = tokenizer.texts_to_sequences(new_sentences)\n",
    "new_padded = pad_sequences(new_sequences, maxlen=100, padding='post', truncating='post')\n",
    "\n",
    "new_labels_encoded = label_encoder.transform(new_labels)\n",
    "new_labels_one_hot = tf.keras.utils.to_categorical(new_labels_encoded, num_classes=len(label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a24def83-ee5d-471d-839c-9b07b5f4271e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 07:16:45.911942: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8400\n",
      "2024-06-14 07:16:49.247077: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-06-14 07:16:49.251794: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-06-14 07:16:49.251880: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-06-14 07:16:49.253654: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-06-14 07:16:49.253733: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2024-06-14 07:16:49.419514: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 11s 11s/step - loss: 0.1694 - accuracy: 1.0000 - val_loss: 0.4143 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.6915 - accuracy: 0.7500 - val_loss: 0.3769 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.1685 - accuracy: 1.0000 - val_loss: 0.3438 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.2527 - accuracy: 1.0000 - val_loss: 0.3112 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.5511 - accuracy: 0.7500 - val_loss: 0.2801 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the loaded model with the new data\n",
    "history = model.fit(new_padded, \n",
    "                    new_labels_one_hot, \n",
    "                    epochs=5,  # A few epochs for fine-tuning\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6f23e-f712-4d5b-a811-0628269194f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated model\n",
    "model.save('text_classify_feedbacked.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9(haidarenv)",
   "language": "python",
   "name": "haidarenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
